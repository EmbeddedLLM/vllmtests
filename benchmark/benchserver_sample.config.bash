#!/bin/bash

# Update these to your result dirs
VLLM_PORT=11169
LOGS_PATH="llama3.1bench/logs/cudagraph_logs_1024"
RESULT_DIR="llama3.1bench/results/cudagraph_results_1024"
HIP_VISIBLE_DEVICES=0

# Define test cases
test_cases=(
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    "meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
    # "meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"
)

# Define environment variables for each test case
env_vars=(
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_USE_AITER_UNIFIED_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_USE_AITER_UNIFIED_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_USE_AITER_UNIFIED_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_USE_AITER_UNIFIED_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_USE_AITER_UNIFIED_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=0 VLLM_V1_USE_PREFILL_DECODE_ATTENTION=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=1"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="VLLM_USE_V1=1 VLLM_ROCM_USE_AITER_RMSNORM=0 VLLM_ROCM_USE_AITER=1 VLLM_ROCM_USE_AITER_MHA=1"
)

# Define model parameters for each test case
model_params=(
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"PIECEWISE\", \"level\": \"3\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL_AND_PIECEWISE\", \"level\": \"3\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL_DECODE_ONLY\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"NONE\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"PIECEWISE\", \"level\": \"3\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL_AND_PIECEWISE\", \"level\": \"3\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL_DECODE_ONLY\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"NONE\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"NONE\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"PIECEWISE\", \"level\": \"3\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL_AND_PIECEWISE\", \"level\": \"3\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL_DECODE_ONLY\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="meta-llama/Llama-3.1-70B-Instruct --tensor-parallel-size 1 --max-model-len 32768 --compilation-config '{\"cudagraph_mode\":\"FULL\"}' --kv-cache-dtype fp8 --async-scheduling --max-num-batched-tokens=1024"
)

folder_name=(
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-unified-attention_cudagraph-mode=FULL_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-unified-attention_cudagraph-mode=PIECEWISE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-unified-attention_cudagraph-mode=FULL_AND_PIECEWISE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-unified-attention_cudagraph-mode=FULL_DECODE_ONLY_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_unified_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-unified-attention_cudagraph-mode=NONE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-prefill-decode-attention_cudagraph-mode=FULL_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-prefill-decode-attention_cudagraph-mode=PIECEWISE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-prefill-decode-attention_cudagraph-mode=FULL_AND_PIECEWISE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-prefill-decode-attention_cudagraph-mode=FULL_DECODE_ONLY_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter triton_prefill_decode_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_triton-prefill-decode-attention_cudagraph-mode=NONE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=NONE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_aiter-multi-head-attention_cudagraph-mode=NONE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_aiter-multi-head-attention_cudagraph-mode=PIECEWISE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_AND_PIECEWISE v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_aiter-multi-head-attention_cudagraph-mode=FULL_AND_PIECEWISE_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL_DECODE_ONLY v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_aiter-multi-head-attention_cudagraph-mode=FULL_DECODE_ONLY_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
    ["meta-llama/Llama-3.1-70B-Instruct aiter aiter_multi_head_attention cudagraph_mode=FULL v1 kvcache=fp8 max-model-length=32768 block-size=32 async-scheduling max-num-batched-tokens=1024"]="llama3.1-70B-Instruct_aiter_aiter-multi-head-attention_cudagraph-mode=FULL_v1_kvcache-fp8_max-model-length=32768_block-size=32_async-scheduling_max-num-batched-tokens=1024"
)
